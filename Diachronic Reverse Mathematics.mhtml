Subject: Diachronic Reverse Mathematics
Date: Tue, 29 Aug 2023 21:50:18 -0000

Summarize: A category theory framework for Bayesian 
learning 
Kotaro Kamiya &amp; John Welliaveetil 
∗ 
SyntheticGestalt Ltd 
{k.kamiya, j.welliaveetil}@syntheticgestalt.com 
Abstract 
Inspired by the foundational works in [7] and [3], we introduce a cate- 
gorical framework to formalize Bayesian inference and learning. The two 
key ideas at play here are the notions of Bayesian inversions and the func- 
tor GL as constructed in [3, §2.1]. We i nd that Bayesian learning is the 
simplest case of the learning paradigm described in [3]. We then obtain 
categorical formulations of batch and sequential Bayes updates while also 
verifying that the two coincide in a specif i c example. 
Contents 
1Introduction2 
2Preliminaries5 
2.1Bayesian Inference . . . . . . . . . . . . . . . . . . . . . . . . . .5 
2.2Bayesian inversions and PS. . . . . . . . . . . . . . . . . . . . .7 
2.2.1The category PS . . . . . . . . . . . . . . . . . . . . . . .10 
2.3The Para construction . . . . . . . . . . . . . . . . . . . . . . . .11 
3Bayes Learn12 
3.1Inducing the M-actegory structure. . . . . . . . . . . . . . . .12 
3.2Bayes Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 
3.2.1The functor Stat . . . . . . . . . . . . . . . . . . . . . . .17 
3.2.2The Grothendieck Lens. . . . . . . . . . . . . . . . . . .17 
3.2.3The functor R . . . . . . . . . . . . . . . . . . . . . . . .17 
3.2.4The functor BayesLearn . . . . . . . . . . . . . . . . . . .18 
3.3Bayes Learning algorithm . . . . . . . . . . . . . . . . . . . . . .19 
4Bayes updates21 
4.1Sequential updates . . . . . . . . . . . . . . . . . . . . . . . . . .21 
4.2Batch updates. . . . . . . . . . . . . . . . . . . . . . . . . . . .24 
∗Authors have equal contribution. 1 
1Introduction 
A standard problem in Machine learning is to understand the relationship be- 
tween two variables or random vectors. Suppose x is a random vector and y 
is a random variable dependent on x. The additive model assumes that there 
exists a function f such that y = f(x) + ǫ 
where ǫ is a random variable with mean 0. Our goal then reduces to estimating 
the function f. To this end, we consider a parametrized family of functions 
{fθ}θ∈Pwhere P is the parameter space and by means of a learning algorithm 
and a training data set, we traverse P to i nd a candidate that we believe will 
provide a reasonable approximation to the true function f. Several theoreti- 
cal results underpin the validity of this approach- for instance the Universal 
